In order to optimize the continuous time models we choose to use reinforcement learning methods, the following section covers the Markov Decision Process formalism and gives an introduction to the main ideas behind Proximal Policy Optimization (PPO) \cite{schulman2017ppo}, the algorithm that we will use to train our model. \\% This formulation has several advantages, first it enables the training of the model to happen without any exterior intervention or training data, and secondly it will enable us to 

\subsection{Markov Decision Processes}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/system.pdf}
    \caption{A visual representation of the different components required to define an RL problem. An environment, which keeps state and defines the transition probability function, and a policy function (which may have a hidden state $h$) which takes observations $o_t$ of $s_t$ as inputs and returns actions $a_t$ as outputs.}
\end{figure}

Formulating our control problem as a reinforcement learning problem requires us to write out a control task as a partially observable Markov decision process (POMDP). We define a POMDP through:
\begin{enumerate}
    \item a (partially observable) state space $\mathcal{S}$ with states $s_t\in \mathbb{R}^n$,
    \item an observation space $\mathcal{O}$ with states $o_t\in \mathbb{R}^m$, where observations $o_t$ give some information about the true states $s_t$,
    \item an action space $\mathcal{A}$ with action $a_t \in \mathcal{A}\subseteq\mathbb{R}^d$, which gives the possible actions that the agent can take in a given state,
    \item an unknown transition probability function $P(s_{t+1}|s_{t},a_{t})$, which gives the probability that the system transitions to state $s_{t+1}$ if it is in state $s_{t}$ and action $a_t$ is taken,
    \item a reward function $R:(s_{t+1},s_t,a_t) \rightarrow \mathbb{R}$.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/MDP.pdf}
    \caption{One can think of a Markov Decision Process as a tree of possible states $s_t, s_{t+1},...$ connected by branches associated with the transition probability function $P(s_{t+1}|s_t,a_t)$.}
\end{figure}

Given a POMDP, a discount factor $\gamma \in [0,1)$ and a policy function $\pi :\mathcal{S}\rightarrow\mathcal{A}$ we can compute the \textit{expected discounted reward} using the Bellman equation: 

\begin{align*}
    J{\pi_\theta} = \mathbb{E} \left[ \sum^\infty_{t=0} \gamma^t R(s_{t+1},s_t,\pi_\theta(o_t)) \right].
\end{align*}

The problem of reinforcement learning is formulated as the optimization of some parametrizable policy function $\pi$ (where the parameters are denoted $\theta$) over the POMDP process that ensures the the $J$ value is maximized in expectation across all states, i.e.:

\begin{align*}
    \pi^* = \text{argmax}_{\pi_\theta} \mathbb{E} \left[ \sum^\infty_{t=0} \gamma^t R(s_{t+1},s_t,\pi_\theta(o_t)) \right].
\end{align*}

\subsection{Policy Gradient Methods}

We will consider a \textit{policy gradient} reinforcement learning method, such an approach is the most natural for a continuous control task, these method work by directly updating a policy function rather than by computing an estimator of the discounted reward for all possible actions (which is what is done in Q-learning methods). Policy gradient RL methods use update rules of the form:

\begin{align*}
    \theta_{k+1} = \theta_k + \alpha \nabla_{\theta_k} J(\pi_{\theta_k}) \vert_{\theta_k}.
\end{align*}

Here the tricky part of the method resides in deriving an expression for the gradient $\nabla_{\theta_k} J(\pi_{\theta_k}) \vert_{\theta_k}$ of the expected reward $J$ with respect to the parameters $\theta$ of the policy $\pi$. These methods are often presented in MDP instead of POMDP form but they generalize well to POMDPs. The derivation of policy gradient is performed as follows:
\begin{align*}
    \nabla_{\theta_k} J(\pi_{\theta_k}) \vert_{\theta_k} 
    =  \nabla_{\theta_k} \int_\tau P(\tau|\theta)R(\tau)
    % && \text{}
    && \text{Expand expectation}\\
    =  \int_\tau \nabla_{\theta_k}  P(\tau|\theta)R(\tau)
    && \text{Use linearity}\\
    =  \int_\tau  P(\tau|\theta) \nabla_{\theta_k} \log P(\tau|\theta)R(\tau)
    && \text{Log trick} \\
    =  \underset{\tau \sim \pi_\theta}{\mathbb{E}} \left[\nabla_{\theta_k} \log P(\tau|\theta)R(\tau)\right]
    && \text{Take the expectations}\\
    =  \underset{\tau \sim \pi_\theta}{\mathbb{E}} \left[\nabla_{\theta_k} \sum_{t=0}^T \log \pi_\theta(a_t|s_t)R(\tau)\right]
    && \text{Expand over trajectories}
\end{align*}

Where $\tau$ denotes the set of all trajectories $s_0,a_0,s_1,a_1,...$ in the state space under policy $\pi_\theta$. In practice one can compute an estimate $\hat{g}$ of $\nabla_{\theta_k} J(\pi_{\theta_k})$ by sampling the MDP a sufficiently large number of time. Such an estimator is written out as follows: 

\begin{align*}
    \hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) G_t,
\end{align*}
where $G_t$ is the expected discounted reward over the remaining steps in the episode.

\subsection{Advantage Actor Critic}

Policy gradient methods derived as we described above tend to lead to unstable learning. 
This is largely attributable to the fact that the gradient norms are subject to a high variance \cite{Konda00actor-criticalgorithms}.
The gradient in the variance of $\nabla_{\theta_k} J(\pi_{\theta_k})$ is proportional to the absolute value of the expected discounted reward over the trajectory $R(\tau)$, we can thus reduce the variance of our gradient estimator by subtracting a baseline to it's reward (this doesn't affect the gradients in expectation and thus the algorithm still converges to the same policy), the most common baseline used in that context is the \textit{on-policy value function $V_{\pi_\theta}(s)$}. 
This means a policy algorithm such as A2C we will have require two separate networks, an actor network (to implement a policy function) and a a critic network (to implement a value estimator) which both need to train simultaneously. To derive the formulation of advantage actor critic we first observe policy gradient implicitely makes use of $Q$ values.\\

\begin{align*}
    \hat{g} = \underset{\tau}{\mathbb{E}} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) G_t \right] && \text{}\\
    = \underset{s_0,a_0,...}{\mathbb{E}} \bigg[ \big[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) G_t \big] && \text{Observe that:}\\
    + \underset{s_{t+1},r_{t+1},...,s_{T},r_{T}}{\mathbb{E}} [G_t] \bigg] && \mathbb{E} [G_t] = Q(s_t,a_t)\\
    = \underset{s_0,a_0,...}{\mathbb{E}} \bigg[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) Q(s_t,a_t) \bigg] && 
\end{align*}

Then we subtract the baseline $V$ as follows:
\begin{align*}
    \hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) (G_t-V(s))\\
\end{align*}

Using the Bellman Optimality equation $Q(s_t,a_t) = \mathbb{E}[r_{t+1}] + \gamma V(s_{t+1})$ we have that can write out the advantage function as:
\begin{align*}
    A(s_{t+1},s_t,a_t) = Q(s_t,a_t) - V(s_t,a_t) \\
   \sim r_{t+1} + \gamma V(s_{t+1}) - V(s_t,a_t) 
\end{align*}

This approach leads us the the \textit{Advantage Actor Critic Method} (A2C) which computes it's gradients from an advantage function $A$ instead of a direct reward $R$:

\begin{align*}
    \hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) A(s_{t+1},s_t,a_t)\\
    = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta (a_t|s_t) (r_{t+1} + \gamma V(s_{t+1}) - V(s_t))
\end{align*}

In that framework we train two networks side by side, one of them is updated by the approximative gradient $\hat{g}$, and the other is updated by some loss function so that it converges to correct $V$ values.

\subsection{Proximal Policy Optimization}
\label{sec:ppo}
\textbf{Describe PPO}

\subsection{An architecture for training CTNNs using PPO}

\textbf{Describe what I did}
